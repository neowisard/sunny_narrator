TARGET_LANG=russian
SOURCE_LANG=english
COUNTRY=Россия
MAX_LEN_CHUNK=4096
#have some degradation if chunks bigger 8192 tokens (not bytes)
FILE='books/Daemon.fb2'
API_KEY=a132b20c-96be-467f-a15a-ed08aed67345
NOTHINK=0
# Mistral 24b or Qwen 30B , If QWEN  nothink=True (1))
API_BASE=http://localhost:11434/v1
TEMP=0
MODEL=gemma3:4b-it-q4_K_M
MODEL2=gemma3:4b-it-q4_K_M
#For REFLECTION AND EDITOR and SYNOPSIS fast , Saiga_gemma12
API_KEY2=a132b20c-96be-467f-a15a-ed08aed67345
API_BASE2=http://localhost:11434/v1
TEMP2=0.1
TIMEOUT=6000
TIMEOUT2=6000
#SPROMT2=0
NOTHINK2=0
#EXAMPLE="\n It's subtitles from SRT file, leave the design and numbering as it is, as example:\n2\n00:00:16,474 --> 00:00:18,476\n[soldier] Did he try\nto break it for you?\n\n3\n00:00:22,063 --> 00:00:24,065\n[chatter stops]"
NER=f  #If True - make automatic vocabulary file via neural lib spaCy, just manual fix it after work - book/BookName.dic
NERMODEL=en_core_web_lg
#VOCAB=for manual vocabulary just create file /book/BookName.dic as in FILE above
DEBUG=1
